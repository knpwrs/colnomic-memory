# Benchmark Results

Generated: 2025-10-10 07:14:22

## COLNOMIC-3B

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 14.88 min | 7.49 GB | 2000 | 2000 |
| 4 | success | 18.73 min | 9.43 GB | 2000 | 500 |
| 8 | success | 25.04 min | 14.69 GB | 2000 | 250 |
| 16 | success | 37.94 min | 34.14 GB | 2000 | 125 |
| 32 | failed | Failed | N/A | N/A | N/A |
| 64 | failed | Failed | N/A | N/A | N/A |
| 128 | failed | Failed | N/A | N/A | N/A |

## COLNOMIC-7B

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 9.54 min | 14.92 GB | 2000 | 2000 |
| 4 | success | 10.37 min | 15.82 GB | 2000 | 500 |
| 8 | success | 12.43 min | 17.69 GB | 2000 | 250 |
| 16 | success | 16.76 min | 24.81 GB | 2000 | 125 |
| 32 | success | 25.74 min | 51.41 GB | 2000 | 63 |
| 64 | failed | Failed | N/A | N/A | N/A |
| 128 | failed | Failed | N/A | N/A | N/A |

## DINOV2-BASE

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 0.57 min | 0.20 GB | 2000 | 2000 |
| 4 | success | 0.44 min | 0.22 GB | 2000 | 500 |
| 8 | success | 0.42 min | 0.24 GB | 2000 | 250 |
| 16 | success | 0.42 min | 0.29 GB | 2000 | 125 |
| 32 | success | 0.41 min | 0.38 GB | 2000 | 63 |
| 64 | success | 0.42 min | 0.56 GB | 2000 | 32 |
| 128 | success | 0.42 min | 0.93 GB | 2000 | 16 |

## DINOV2-GIANT

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 0.89 min | 2.16 GB | 2000 | 2000 |
| 4 | success | 0.52 min | 2.20 GB | 2000 | 500 |
| 8 | success | 0.48 min | 2.25 GB | 2000 | 250 |
| 16 | success | 0.47 min | 2.35 GB | 2000 | 125 |
| 32 | success | 0.47 min | 2.56 GB | 2000 | 63 |
| 64 | success | 0.47 min | 2.97 GB | 2000 | 32 |
| 128 | success | 0.47 min | 3.79 GB | 2000 | 16 |

## DINOV2-LARGE

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 0.70 min | 0.61 GB | 2000 | 2000 |
| 4 | success | 0.47 min | 0.63 GB | 2000 | 500 |
| 8 | success | 0.44 min | 0.66 GB | 2000 | 250 |
| 16 | success | 0.43 min | 0.72 GB | 2000 | 125 |
| 32 | success | 0.42 min | 0.84 GB | 2000 | 63 |
| 64 | success | 0.42 min | 1.07 GB | 2000 | 32 |
| 128 | success | 0.43 min | 1.55 GB | 2000 | 16 |

## DINOV2-SMALL

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 0.58 min | 0.08 GB | 2000 | 2000 |
| 4 | success | 0.45 min | 0.09 GB | 2000 | 500 |
| 8 | success | 0.43 min | 0.10 GB | 2000 | 250 |
| 16 | success | 0.42 min | 0.12 GB | 2000 | 125 |
| 32 | success | 0.41 min | 0.17 GB | 2000 | 63 |
| 64 | success | 0.41 min | 0.27 GB | 2000 | 32 |
| 128 | success | 0.41 min | 0.48 GB | 2000 | 16 |

## NOMIC-3B

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 9.37 min | 7.35 GB | 2000 | 2000 |
| 4 | success | 10.04 min | 8.15 GB | 2000 | 500 |
| 8 | success | 12.04 min | 10.18 GB | 2000 | 250 |
| 16 | success | 16.40 min | 17.28 GB | 2000 | 125 |
| 32 | success | 25.44 min | 43.85 GB | 2000 | 63 |
| 64 | failed | Failed | N/A | N/A | N/A |
| 128 | failed | Failed | N/A | N/A | N/A |

## NOMIC-7B

| Batch Size | Status | Runtime | Peak VRAM | Images | Batches |
|------------|--------|---------|-----------|--------|---------|
| 1 | success | 9.05 min | 14.92 GB | 2000 | 2000 |
| 4 | success | 10.33 min | 15.81 GB | 2000 | 500 |
| 8 | success | 12.32 min | 17.68 GB | 2000 | 250 |
| 16 | success | 16.80 min | 24.80 GB | 2000 | 125 |
| 32 | success | 25.72 min | 51.40 GB | 2000 | 63 |
| 64 | failed | Failed | N/A | N/A | N/A |
| 128 | failed | Failed | N/A | N/A | N/A |

## Detailed Results (JSON)

```json
[
  {
    "model_variant": "colnomic-7b",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 629.0712602138519,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 572.28,
    "runtime_minutes": 9.54,
    "peak_memory_gb": 14.92,
    "peak_memory_mb": 15281.41
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 651.7529480457306,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 622.08,
    "runtime_minutes": 10.37,
    "peak_memory_gb": 15.82,
    "peak_memory_mb": 16202.04
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 774.964839220047,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 745.85,
    "runtime_minutes": 12.43,
    "peak_memory_gb": 17.69,
    "peak_memory_mb": 18110.91
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 1034.039909362793,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 1005.63,
    "runtime_minutes": 16.76,
    "peak_memory_gb": 24.81,
    "peak_memory_mb": 25400.39
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 1572.632515668869,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 1544.48,
    "runtime_minutes": 25.74,
    "peak_memory_gb": 51.41,
    "peak_memory_mb": 52643.63
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 64,
    "status": "failed",
    "error": "\nFetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\nFetching 7 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:00<00:00, 118387.61it/s]\n\nLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\nLoading checkpoint shards:  14%|\u2588\u258d        | 1/7 [00:00<00:05,  1.16it/s]\nLoading checkpoint shards:  29%|\u2588\u2588\u258a       | 2/7 [00:01<00:04,  1.22it/s]\nLoading checkpoint shards:  43%|\u2588\u2588\u2588\u2588\u258e     | 3/7 [00:02<00:03,  1.25it/s]\nLoading checkpoint shards:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4/7 [00:03<00:02,  1.26it/s]\nLoading checkpoint shards:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 5/7 [00:03<00:01,  1.27it/s]\nLoading checkpoint shards:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:04<00:00,  1.27it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:04<00:00,  1.69it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:04<00:00,  1.41it/s]\nSome weights of the model checkpoint at nomic-ai/colqwen2.5-7B-base were not used when initializing ColQwen2_5: ['lm_head.weight']\n- This IS expected if you are initializing ColQwen2_5 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ColQwen2_5 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 65.37 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.30 GiB is free. Including non-PyTorch memory, this process has 18.87 GiB memory in use. Of the allocated memory 17.66 GiB is allocated by PyTorch, and 550.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 34.216803789138794
  },
  {
    "model_variant": "colnomic-7b",
    "batch_size": 128,
    "status": "failed",
    "error": "\nFetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\nFetching 7 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:00<00:00, 113798.95it/s]\n\nLoading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\nLoading checkpoint shards:  14%|\u2588\u258d        | 1/7 [00:00<00:05,  1.11it/s]\nLoading checkpoint shards:  29%|\u2588\u2588\u258a       | 2/7 [00:01<00:04,  1.17it/s]\nLoading checkpoint shards:  43%|\u2588\u2588\u2588\u2588\u258e     | 3/7 [00:02<00:03,  1.22it/s]\nLoading checkpoint shards:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4/7 [00:03<00:02,  1.24it/s]\nLoading checkpoint shards:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 5/7 [00:04<00:01,  1.26it/s]\nLoading checkpoint shards:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:04<00:00,  1.27it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:05<00:00,  1.69it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:05<00:00,  1.39it/s]\nSome weights of the model checkpoint at nomic-ai/colqwen2.5-7B-base were not used when initializing ColQwen2_5: ['lm_head.weight']\n- This IS expected if you are initializing ColQwen2_5 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ColQwen2_5 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 261.62 GiB. GPU 0 has a total capacity of 79.18 GiB of which 56.98 GiB is free. Including non-PyTorch memory, this process has 22.19 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 804.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 40.440937519073486
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 928.3995850086212,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 892.93,
    "runtime_minutes": 14.88,
    "peak_memory_gb": 7.49,
    "peak_memory_mb": 7666.7
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 1148.8449540138245,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 1123.68,
    "runtime_minutes": 18.73,
    "peak_memory_gb": 9.43,
    "peak_memory_mb": 9659.27
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 1527.1287140846252,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 1502.52,
    "runtime_minutes": 25.04,
    "peak_memory_gb": 14.69,
    "peak_memory_mb": 15041.7
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 2300.9388144016266,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 2276.56,
    "runtime_minutes": 37.94,
    "peak_memory_gb": 34.14,
    "peak_memory_mb": 34958.17
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 32,
    "status": "failed",
    "error": "\nFetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\nFetching 2 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 41943.04it/s]\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\nLoading checkpoint shards:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.05it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.56it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.45it/s]\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 492, in forward\n    hidden_states = blk(\n                    ^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 286, in forward\n    hidden_states = hidden_states + self.attn(\n                                    ^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 248, in forward\n    attn_output, _ = attention_interface(\n                     ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py\", line 66, in sdpa_attention_forward\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 47.36 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.78 GiB is free. Including non-PyTorch memory, this process has 61.39 GiB memory in use. Of the allocated memory 60.39 GiB is allocated by PyTorch, and 270.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 29.67168354988098
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 64,
    "status": "failed",
    "error": "\nFetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\nFetching 2 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 41943.04it/s]\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\nLoading checkpoint shards:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.04it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.54it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.43it/s]\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 189.39 GiB. GPU 0 has a total capacity of 79.18 GiB of which 65.94 GiB is free. Including non-PyTorch memory, this process has 13.23 GiB memory in use. Of the allocated memory 12.05 GiB is allocated by PyTorch, and 520.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 32.74290347099304
  },
  {
    "model_variant": "colnomic-3b",
    "batch_size": 128,
    "status": "failed",
    "error": "\nFetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\nFetching 2 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 42366.71it/s]\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\nLoading checkpoint shards:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.02it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.51it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.41it/s]\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 755.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.56 GiB is free. Including non-PyTorch memory, this process has 18.61 GiB memory in use. Of the allocated memory 16.98 GiB is allocated by PyTorch, and 973.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 41.83825635910034
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 582.2275114059448,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 542.78,
    "runtime_minutes": 9.05,
    "peak_memory_gb": 14.92,
    "peak_memory_mb": 15273.59
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 646.3559386730194,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 619.64,
    "runtime_minutes": 10.33,
    "peak_memory_gb": 15.81,
    "peak_memory_mb": 16194.22
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 765.6175546646118,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 739.39,
    "runtime_minutes": 12.32,
    "peak_memory_gb": 17.68,
    "peak_memory_mb": 18103.09
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 1033.2838914394379,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 1007.7,
    "runtime_minutes": 16.8,
    "peak_memory_gb": 24.8,
    "peak_memory_mb": 25392.57
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 1569.6178834438324,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 1543.48,
    "runtime_minutes": 25.72,
    "peak_memory_gb": 51.4,
    "peak_memory_mb": 52635.81
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 64,
    "status": "failed",
    "error": "\nFetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]\nFetching 5 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 95325.09it/s]\n\nLoading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\nLoading checkpoint shards:  20%|\u2588\u2588        | 1/5 [00:00<00:02,  1.41it/s]\nLoading checkpoint shards:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:01<00:01,  1.54it/s]\nLoading checkpoint shards:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:01<00:01,  1.57it/s]\nLoading checkpoint shards:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:02<00:00,  1.60it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02<00:00,  1.96it/s]\nSome weights of ColQwen2_5 were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading adapter weights from nomic-ai/nomic-embed-multimodal-7b led to missing keys in the model: custom_text_proj.lora_A.default.weight, custom_text_proj.lora_B.default.weight\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 65.37 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.58 GiB is free. Including non-PyTorch memory, this process has 18.59 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 267.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 31.609044790267944
  },
  {
    "model_variant": "nomic-7b",
    "batch_size": 128,
    "status": "failed",
    "error": "\nFetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]\nFetching 5 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 70611.18it/s]\n\nLoading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\nLoading checkpoint shards:  20%|\u2588\u2588        | 1/5 [00:00<00:02,  1.42it/s]\nLoading checkpoint shards:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:01<00:01,  1.55it/s]\nLoading checkpoint shards:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:01<00:01,  1.58it/s]\nLoading checkpoint shards:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:02<00:00,  1.60it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02<00:00,  1.97it/s]\nSome weights of ColQwen2_5 were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading adapter weights from nomic-ai/nomic-embed-multimodal-7b led to missing keys in the model: custom_text_proj.lora_A.default.weight, custom_text_proj.lora_B.default.weight\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 261.62 GiB. GPU 0 has a total capacity of 79.18 GiB of which 57.26 GiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 522.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 37.13911008834839
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 598.2395973205566,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 561.99,
    "runtime_minutes": 9.37,
    "peak_memory_gb": 7.35,
    "peak_memory_mb": 7527.61
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 627.1105093955994,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 602.35,
    "runtime_minutes": 10.04,
    "peak_memory_gb": 8.15,
    "peak_memory_mb": 8341.34
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 746.9502091407776,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 722.19,
    "runtime_minutes": 12.04,
    "peak_memory_gb": 10.18,
    "peak_memory_mb": 10427.25
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 1008.3366467952728,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 983.71,
    "runtime_minutes": 16.4,
    "peak_memory_gb": 17.28,
    "peak_memory_mb": 17697.68
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 1550.7395205497742,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 1526.56,
    "runtime_minutes": 25.44,
    "peak_memory_gb": 43.85,
    "peak_memory_mb": 44905.7
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 64,
    "status": "failed",
    "error": "\nFetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\nFetching 2 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 38836.15it/s]\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\nLoading checkpoint shards:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.30it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.47it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.44it/s]\nSome weights of ColQwen2_5 were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading adapter weights from nomic-ai/nomic-embed-multimodal-3b led to missing keys in the model: custom_text_proj.lora_A.default.weight, custom_text_proj.lora_B.default.weight\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 492, in forward\n    hidden_states = blk(\n                    ^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 286, in forward\n    hidden_states = hidden_states + self.attn(\n                                    ^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 237, in forward\n    query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 151, in apply_rotary_pos_emb_vision\n    q, k = q.float(), k.float()\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 916.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 474.25 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 77.22 GiB is allocated by PyTorch, and 768.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 30.56400752067566
  },
  {
    "model_variant": "nomic-3b",
    "batch_size": 128,
    "status": "failed",
    "error": "\nFetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]\nFetching 2 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 41734.37it/s]\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\nLoading checkpoint shards:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.30it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.47it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.44it/s]\nSome weights of ColQwen2_5 were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading adapter weights from nomic-ai/nomic-embed-multimodal-3b led to missing keys in the model: custom_text_proj.lora_A.default.weight, custom_text_proj.lora_B.default.weight\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\nTraceback (most recent call last):\n  File \"/root/nomic-memory/main.py\", line 420, in <module>\n    main()\n  File \"/root/nomic-memory/main.py\", line 393, in main\n    embeddings = process_images_in_batches(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/main.py\", line 219, in process_images_in_batches\n    embeddings = model(**current_inputs_gpu)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py\", line 50, in forward\n    super()\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1250, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1200, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 491, in forward\n    attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/nomic-memory/.venv/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 434, in _prepare_attention_mask\n    attention_mask = torch.full(\n                     ^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 261.62 GiB. GPU 0 has a total capacity of 79.18 GiB of which 64.96 GiB is free. Including non-PyTorch memory, this process has 14.21 GiB memory in use. Of the allocated memory 12.96 GiB is allocated by PyTorch, and 594.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "wall_time_seconds": 40.961923122406006
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 56.96695828437805,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 34.81,
    "runtime_minutes": 0.58,
    "peak_memory_gb": 0.08,
    "peak_memory_mb": 78.75
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 48.64141583442688,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 27.06,
    "runtime_minutes": 0.45,
    "peak_memory_gb": 0.09,
    "peak_memory_mb": 88.01
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 46.987388134002686,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 25.63,
    "runtime_minutes": 0.43,
    "peak_memory_gb": 0.1,
    "peak_memory_mb": 101.57
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 46.46752381324768,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 25.05,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.12,
    "peak_memory_mb": 126.45
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 45.816810846328735,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 24.64,
    "runtime_minutes": 0.41,
    "peak_memory_gb": 0.17,
    "peak_memory_mb": 177.8
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 64,
    "status": "success",
    "wall_time_seconds": 46.60251498222351,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 32,
    "runtime_seconds": 24.85,
    "runtime_minutes": 0.41,
    "peak_memory_gb": 0.27,
    "peak_memory_mb": 280.51
  },
  {
    "model_variant": "dinov2-small",
    "batch_size": 128,
    "status": "success",
    "wall_time_seconds": 46.694525957107544,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 16,
    "runtime_seconds": 24.9,
    "runtime_minutes": 0.41,
    "peak_memory_gb": 0.48,
    "peak_memory_mb": 486.41
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 56.068562746047974,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 34.13,
    "runtime_minutes": 0.57,
    "peak_memory_gb": 0.2,
    "peak_memory_mb": 209.15
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 48.030372858047485,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 26.65,
    "runtime_minutes": 0.44,
    "peak_memory_gb": 0.22,
    "peak_memory_mb": 226.64
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 46.5625262260437,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 25.39,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.24,
    "peak_memory_mb": 250.39
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 46.34703874588013,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 25.15,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.29,
    "peak_memory_mb": 296.78
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 46.380404472351074,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 24.67,
    "runtime_minutes": 0.41,
    "peak_memory_gb": 0.38,
    "peak_memory_mb": 390.3
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 64,
    "status": "success",
    "wall_time_seconds": 46.20402526855469,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 32,
    "runtime_seconds": 25.23,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.56,
    "peak_memory_mb": 577.33
  },
  {
    "model_variant": "dinov2-base",
    "batch_size": 128,
    "status": "success",
    "wall_time_seconds": 46.49760437011719,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 16,
    "runtime_seconds": 25.1,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.93,
    "peak_memory_mb": 951.89
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 66.09947609901428,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 42.03,
    "runtime_minutes": 0.7,
    "peak_memory_gb": 0.61,
    "peak_memory_mb": 620.36
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 49.76177096366882,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 28.44,
    "runtime_minutes": 0.47,
    "peak_memory_gb": 0.63,
    "peak_memory_mb": 643.12
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 48.148375272750854,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 26.37,
    "runtime_minutes": 0.44,
    "peak_memory_gb": 0.66,
    "peak_memory_mb": 673.53
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 46.49477696418762,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 25.56,
    "runtime_minutes": 0.43,
    "peak_memory_gb": 0.72,
    "peak_memory_mb": 734.34
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 46.36335825920105,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 25.49,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 0.84,
    "peak_memory_mb": 855.96
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 64,
    "status": "success",
    "wall_time_seconds": 46.44468951225281,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 32,
    "runtime_seconds": 25.45,
    "runtime_minutes": 0.42,
    "peak_memory_gb": 1.07,
    "peak_memory_mb": 1099.21
  },
  {
    "model_variant": "dinov2-large",
    "batch_size": 128,
    "status": "success",
    "wall_time_seconds": 47.36218547821045,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 16,
    "runtime_seconds": 25.52,
    "runtime_minutes": 0.43,
    "peak_memory_gb": 1.55,
    "peak_memory_mb": 1588.21
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 1,
    "status": "success",
    "wall_time_seconds": 79.25514912605286,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 2000,
    "runtime_seconds": 53.47,
    "runtime_minutes": 0.89,
    "peak_memory_gb": 2.16,
    "peak_memory_mb": 2213.55
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 4,
    "status": "success",
    "wall_time_seconds": 52.44106078147888,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 500,
    "runtime_seconds": 31.14,
    "runtime_minutes": 0.52,
    "peak_memory_gb": 2.2,
    "peak_memory_mb": 2254.49
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 8,
    "status": "success",
    "wall_time_seconds": 50.90617108345032,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 250,
    "runtime_seconds": 29.08,
    "runtime_minutes": 0.48,
    "peak_memory_gb": 2.25,
    "peak_memory_mb": 2305.41
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 16,
    "status": "success",
    "wall_time_seconds": 49.54682946205139,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 125,
    "runtime_seconds": 28.12,
    "runtime_minutes": 0.47,
    "peak_memory_gb": 2.35,
    "peak_memory_mb": 2410.4
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 32,
    "status": "success",
    "wall_time_seconds": 49.426308393478394,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 63,
    "runtime_seconds": 27.91,
    "runtime_minutes": 0.47,
    "peak_memory_gb": 2.56,
    "peak_memory_mb": 2620.37
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 64,
    "status": "success",
    "wall_time_seconds": 49.05239462852478,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 32,
    "runtime_seconds": 27.92,
    "runtime_minutes": 0.47,
    "peak_memory_gb": 2.97,
    "peak_memory_mb": 3041.3
  },
  {
    "model_variant": "dinov2-giant",
    "batch_size": 128,
    "status": "success",
    "wall_time_seconds": 49.46219229698181,
    "backend": "cuda",
    "total_images": 2000,
    "total_batches": 16,
    "runtime_seconds": 27.91,
    "runtime_minutes": 0.47,
    "peak_memory_gb": 3.79,
    "peak_memory_mb": 3882.68
  }
]
```